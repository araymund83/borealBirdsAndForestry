---
title: "New Analysis Suarez et al."
author: "Tati Micheletti"
date: "January 24, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  message = FALSE,  warning = FALSE)
```

### New analysis for manuscript reviewers

After the first round of reviews, these concerns came back as major revisions:

1. lines 119-121: I disagree with your assumption that Boreal Ecosystem
  Anthropogenic Disturbance polygons that did not intersect with the Global
  Forest Change maps were disturbances that happened before 2000 (the first
  year of the latter data set), just because the two products were mapped
  at the same spatial resolution. This is problematic because it also means
  you are implicitly assuming zero (or negligible) classification error in
  each data set.  A lack of intersection could also occur because of
  omission/commission classification errors in the data sets. The methods
  used to classify disturbance and forest loss in these data sets,
  respectively, are very different. Thus I think it reasonable to expect
  that classification errors would differ between the two data sets. At
  minimum, I think there needs to be more thought (and discussion in the
  paper) about this assumption and how it could potentially impact the
  results. I also recommend you conduct supplementary analyses to
  demonstrate that your results are robust to this assumption.
  
2. lines 204-208: I assume that you used different subsets of your data
  to evaluate the relative importance of successional and alienating
  disturbances in an attempt to disentangle their effects (i.e. combat high
  collinearity between these disturbance types); this reasoning should be
  made more clear in the methods. Based on this assumption, I have two main
  concerns with this approach. First, it is not clear whether successional
  and alienating disturbances were also highly correlated in your two data
  subsets. This is because your subsetting process could still allow for
  enough variation in both alienating and successional disturbances across
  the subset of landscapes to have correlations between the two variables.
  For example, if a buffer had 51% alienating disturbance (the predominant
  disturbance type) it could still have up to 49% successional disturbance.
  If these are still correlated in your subsets, then interpretation of the
  relative importance is problematic because you have not controlled for
  the collinearity in your analysis. Second, even if this approach reduced
  the correlation between the two types of disturbance, I am concerned that
  subsetting the data introduces factors that could confound your
  interpretation of the relative importance of alienating and successional
  disturbances, because you are testing these effects with different data
  sets. For example, all else being equal, you'd expect to be less likely
  to detect a significant effect when using a smaller data set than using a
  larger one. Also, subsetting the data reduced the range of variation in
  your variables and this could affect the relative importance estimates if
  subsetting resulted in a larger decline in the range of variation in the
  target disturbance measure in one subset vs. the other. This is because,
  all else being equal, you'd expect to detect a stronger effect when the
  range of variation is larger. I suggest you consider these issues, and
  suggest that it would likely be better to test for relative importance by
  modeling bird density as a function of both alienating and successional
  disturbance using the full data set. Previous study suggests that
  standardized partial regression coefficients generally provide unbiased
  estimates of the relative importance of two predictor variables, even
  when predictors are correlated: see Smith et al. 2009. Confronting
  collinearity: comparing methods for disentangling the effects of habitat
  loss and fragmentation, Landscape Ecology 24:1271-1285.
  
*In a nutshell...*

### Question 1

**Question 1)**: Was the assumption that any disturbance polygons  Boreal Ecosystem 
Anthropogenic Disturbance polygons that did not intersect with the 
Global Forest Change maps were disturbances that happened before 2000 
reasonable?

**Method:** We aligned Alberto's We overlayed Alberto's GIS with White 
and Wulder product and the Calling Lake Fiona provided us with.

**Results**:
[Gif showing the thee datasets overlay](https://raw.githubusercontent.com/tati-micheletti/borealBirdsAndForestry/finalAdjustments/outputs/Tati_SuarezWW.gif)

**Conclusion**: We found that at least for Calling Lake the areas that were defined 
as 'pre 2000' in Alberto's GIS (the one used for the submitted manuscript) 
match relatively well with White and Wulder's disturbance product 
(which covers 1985-2011) for the same period as well as the map Fiona 
sent us with the dated disturbances there.

### Question 2

**Question 2)**:  
i) Are successional and alienating disturbances highly 
correlated?  
ii) Even if not, was the range of variation in
  the variables reduced?  
iii) modeling bird density as a function of 
  both alienating and successional disturbance using the full data set 
  (to test for relative importance).  

**Method:** 
i) I tried both the `cor.test` function and a linear model with one 
disturbance as the predictor, and the other as the response `lm`.  
ii) For checking the range of variation, I plotted the distribution of both
variables, for both the whole dataset and for the subsets. At laste, I 
plotted the difference between the variables, excluding when both are 0.  
iii) We will not be able to redo the models for this paper, but this will 
be taken into account on future works.

**Results**:
i and ii) The p-values for both correlation test and the linear model were significant. 
The R^2, however was low on both cases (`lm` = 0.0229; `cor.test` = -0.15). 

```{r Q2i, echo=FALSE}
library("data.table")
newDataset <- fread(file = file.path(getwd(), "Minidataset_master.csv"))

# Testing for correlation between both disturbances
newDataset[, c("percSuccess", "percAliena") := list(Success_L_A/Area_100, Alien_L_A/Area_100)]
local <- newDataset[, .(percSuccess, percAliena)]
summary(local)
library("GGally")
ggpairs(local)

localCorr <- cor.test(local$percSuccess, local$percAliena) # Probably not the best test! Sample is way too big, it will always be significant... But the R2 = -0.1514545
mod <- lm(formula = percAliena ~ percSuccess, data = local)
summary(mod) # Note R-squared of 0.0229

# After subsetting
combinations <- c("localTransitional", "neighborhoodTransitional", "LocalUndisturbedTransitional", 
"localPermanent", "neighborhoodPermanent", "LocalUndisturbedPermanent", 
"localBoth", "neighborhoodBoth", "LocalUndisturbedBoth")

   sDataset <- lapply(
    X = combinations,
    FUN = function(x) {
      state <- ifelse(grepl("local", x), "State_P_100", "State_P_500")
      stateLetter <- ifelse(state == "State_P_100", "L", "N")
      agent <- ifelse(grepl("Permanent", x),
                      "Permanent",
                      ifelse(grepl("Transitional", x), "Transitional", "Both"))
      undist <- ifelse(grepl("Undisturbed", x), TRUE, FALSE)
      both <- ifelse(grepl("Both", x), TRUE, FALSE)
      dataUploaded <- if (undist == TRUE & both == TRUE) {
        newDataset[State_P_100 == 0]
      } else {
        if (undist == TRUE & both == FALSE) {
          newDataset[State_P_100 == 0 &
                     (get(paste0("Agent_", stateLetter)) == agent |
                        get(paste0("Agent_", stateLetter)) == "")]
        } else {
          if (undist == FALSE & both == TRUE) {
            newDataset
          } else {
            newDataset[get(state) == 0 |
                       get(paste0("Agent_", stateLetter)) == agent]
          }
        }
      }
      return(dataUploaded)
    }
  )
   names(sDataset) <- combinations
   
listSubset <- lapply(X = sDataset, FUN = function(subs){
graph <- ggpairs(subs[,.(subs$percSuccess, subs$percAliena)])
cor.val <- localCorr <- cor.test(subs$percSuccess, subs$percAliena) 
mod <- lm(formula = percAliena ~ percSuccess, data = subs)

return(list(graph = graph, correlation = cor.val, linearModel = summary(mod)))
})
names(listSubset) <- combinations

tableCor <- data.frame()
tableCor <- lapply(X = seq(1:length(listSubset)), FUN = function(subs){
   tableCor[1, "subset"] <- names(listSubset)[[subs]]
   tableCor[1, "corr"] <- listSubset[[subs]]$correlation$estimate
   tableCor[1, "corr.sign"] <- listSubset[[subs]]$correlation$p.value
   tableCor[1, "r2.linMod"] <- listSubset[[subs]]$linearModel$adj.r.squared
   tableCor[1, "signif.linMod"] <- tryCatch(listSubset[[subs]]$linearModel$coefficients["percSuccess", "Pr(>|t|)"], 
                                               error = function(e) NA)
   return(tableCor)
})
tableCor <- rbindlist(tableCor)
knitr::kable(tableCor)
```

iii) Was not performed.


**Conclusion**: We found that  
i) The R^2 of both the correlation test and the linear model were low for 
both the whole dataset and the subsets, but still significant. It would be, 
however, hard not to have significancy considering the size of the dataset.
I believe we can trust the R^2 value in this sense.  
ii) The range of both disturbances was similar (they were similarly distributed as well)  
iii) We should deprecate claims about "relative importance" in the text.  




